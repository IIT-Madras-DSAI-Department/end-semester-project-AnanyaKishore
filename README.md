[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/R05VM8Rg)
# IIT-Madras-DA2401-Machine-Learning-Lab-End-Semester-Project

### **Scope:**
This repository contains a complete implementation for 
* Softmax Regression
* the XGBoost Classifier (as per `https://arxiv.org/pdf/1603.02754`)
* the Random Forest Classifier, as implemented in class.
* k Nearest Neighbours (KNN)
  
These algorithms are finally combined, after training, to form a weighted ensemble.

A brief overview of the above mentioned classification algorithms is given below.
### Softmax Regression (Multinomial Logistic Regression)
* `Mechanism` Calculates a score for each class as a linear combination of input features, and then uses the softmax function to convert these scores into a probability distribution over the classes, ensuring the probabilities sum to 1.
* `Advantages` Provides class probabilities that are easy to interpret. Simple and efficient to train; ideal for problems where each data point belongs to exactly one of multiple classes.
* `Disadvantages` Assumes linear decision boundaries, which may underperform on complex, non-linearly separable data. Sensitive to irrelevant features.
### XGBoost Classifier (eXtreme Gradient Boosting)
* `Mechanism` Builds an ensemble of decision trees in an additive manner. Each new tree is fitted to the residuals (gradient) of the loss from the previous ensemble. Uses a regularised objective to control overfitting.
* `Advantages` Yields very high accuracy and strong performance on structured data. Can be adapted to multi-class classification.
* `Disadvantages` More complex to tune. Training is more involved when compared to simpler models. 
### Random Forest Classifier
* `Mechanism` An ensemble of many decision trees built via bagging and random feature selection at each split, then aggregating via majority vote.
* `Advantages` Less prone to overfitting compared to a single decision tree; handles large datasets well.
* `Disadvantages` Bagging reduces interpretability. Might not capture class-boundary relationships precisely.
### k Nearest Neighbours (KNN)
* `Mechanism` A non-parametric algorithm; for a new data point, it finds the 'k' closest data points in the training set (based on a distance metric like Euclidean distance). Assigns the new point to the class that is most common among its k neighbors (majority vote).
* `Advantages` Simple to understand and implement, with no training phase. Can model non-linear decision boundaries effectively.
* `Disadvantages` Computationally expensive for large datasets during prediction time, as it calculates the distance to every training point. Highly susceptible to `curse of dimensionality`. Requires feature scaling to prevent features with large values from dominating the distance calculation.
---
## üìÅ Repository Structure
* `.github/` - Contains GitHub-related configuration files, including feedback and workflow settings from GitHub Classroom.
* `MNIST_train.csv` - Dataset on which the classification algorithms are trained.
* `MNIST_validation.csv` - Dataset on which the classification algorithms are validated to assess their performance (here, F1 score).
* `README.md` - Markdown file describing the repository contents, usage, instructions for implementation, and a sample output.
* `EndSemesterProject_MNISTDigitClassification.pdf` - LaTeX-generated report containing the models used, system architecture, individual model performances, hyperparameter tuning efforts, system optimization mechanisms and detailed thoughts and observations from this exercise.
* `algorithms.py` - Python script containing the implementations for all algorithms used in the system.
* `main.py` - Python script that contains the ensemble architecture, trains the models on the given dataset, and predicts the classes.
---
## üì¶ Installation & Dependencies
### A. Dependencies
* `numpy` == 2.3.3
* `pandas` == 2.3.2
* `scikit-learn` == 1.7.2 (to only avail the use of the `accuracy_score` and `f1_score` functions)
* `scipy` == 1.16.2
### B. Installation
The exact versions can be installed with pip: 
* `pip install numpy==2.3.3 pandas==2.3.2 scikit-learn==1.7.2 scipy == 1.16.2`
---
## ‚ñ∂Ô∏è Running the Code
All experiments are runnable from the command line.
### A. Command-line
* Ensure all dependencies are installed.
* Download the following files:
  * `MNIST_train.csv`
  * `MNIST_validation.csv`
  * `algorithms.py`
  * `main.py`
* Make sure all files are placed in the same directory.
* Open a terminal and navigate to the directory in question.
* To train the models and predict the classes, the Python script of concern is `main.py`.
  * In order to run this script, it is necessary to have `algorithms.py` also downloaded into the same directory.
  * This is because the function `read_data()` and the classes `PCAModel()`, `BaggingSoftmaxClassifier()`, `XGBoostMultiClassifier()`, `RandomForest()` and `KNNClassifier()` are called from `algorithms.py`.
* After the requisite measures are taken, on your terminal, run `python main.py`
* The script does not require any user input.
* To evaluate these algorithms on `MNIST_test.csv`, the user is required to make slight modifications to the code:
  * The user may place `MNIST_test.csv` in the same directory, and change `MNIST_validation.csv` to `MNIST_test.csv` in `main.py` at the following location:
	* ***line 72***, `Xtrain, ytrain, Xval, yval = read_data('MNIST_train.csv', 'MNIST_validation.csv')`
  * The user can modify all occurrences of the variable names *Xval*, *yval* to *Xtest*, *ytest*, if they wish to. 
---
## Sample Output
* Sample output for `main.py`:
  
```text
Beginning full ensemble training using Softmax Regression, XGBoost Classifier, Random Forest Classifier & KNN:
The ensemble weights are distributed as:
  Softmax: 35.00%
  XGBoost: 10.00%
  RandomForest: 10.00%
  KNN: 45.00%

Training model Softmax:
  Beggining bagging for the Softmax Classifier with: {'learning_rate': 0.05, 'n_epochs': 200, 'mini_batch_size': 256}
    Training model 1/10...
    Training model 2/10...
    Training model 3/10...
    Training model 4/10...
    Training model 5/10...
    Training model 6/10...
    Training model 7/10...
    Training model 8/10...
    Training model 9/10...
    Training model 10/10...
Training time for Softmax: 25.65510630607605s

Training model XGBoost:
  Training the XGBoost Classifier with n_estimators: 30, learning_rate: 0.1, max_depth: 3, colsample_bytree: 0.5, subsample: 0.8
    Building tree 1/30...
    Building tree 2/30...
    Building tree 3/30...
    Building tree 4/30...
    Building tree 5/30...
    Building tree 6/30...
    Building tree 7/30...
    Building tree 8/30...
    Building tree 9/30...
    Building tree 10/30...
    Building tree 11/30...
    Building tree 12/30...
    Building tree 13/30...
    Building tree 14/30...
    Building tree 15/30...
    Building tree 16/30...
    Building tree 17/30...
    Building tree 18/30...
    Building tree 19/30...
    Building tree 20/30...
    Building tree 21/30...
    Building tree 22/30...
    Building tree 23/30...
    Building tree 24/30...
    Building tree 25/30...
    Building tree 26/30...
    Building tree 27/30...
    Building tree 28/30...
    Building tree 29/30...
    Building tree 30/30...
Training time for XGBoost: 118.46399593353271s

Training model RandomForest:
  Training the Random Forest Classifier with n_trees: 50, max_depth: 8, max_features: 40:
    Building tree 1/50...
    Building tree 2/50...
    Building tree 3/50...
    Building tree 4/50...
    Building tree 5/50...
    Building tree 6/50...
    Building tree 7/50...
    Building tree 8/50...
    Building tree 9/50...
    Building tree 10/50...
    Building tree 11/50...
    Building tree 12/50...
    Building tree 13/50...
    Building tree 14/50...
    Building tree 15/50...
    Building tree 16/50...
    Building tree 17/50...
    Building tree 18/50...
    Building tree 19/50...
    Building tree 20/50...
    Building tree 21/50...
    Building tree 22/50...
    Building tree 23/50...
    Building tree 24/50...
    Building tree 25/50...
    Building tree 26/50...
    Building tree 27/50...
    Building tree 28/50...
    Building tree 29/50...
    Building tree 30/50...
    Building tree 31/50...
    Building tree 32/50...
    Building tree 33/50...
    Building tree 34/50...
    Building tree 35/50...
    Building tree 36/50...
    Building tree 37/50...
    Building tree 38/50...
    Building tree 39/50...
    Building tree 40/50...
    Building tree 41/50...
    Building tree 42/50...
    Building tree 43/50...
    Building tree 44/50...
    Building tree 45/50...
    Building tree 46/50...
    Building tree 47/50...
    Building tree 48/50...
    Building tree 49/50...
    Building tree 50/50...
Training time for RandomForest: 65.97211408615112s

Training model KNN:
  Note: this may take some time! (~30 seconds)
Training time for KNN: 0.3097550868988037s
Full ensemble training complete in: 210.40s!
Validating now on the training and validation data:

Validating on training data via hard voting...
Note: this may take some time! (~100 seconds)
  Time taken: 81.40752935409546s
  Accuracy: 0.9645070985802839
  F1-Score: 0.9643897922408824

Validating on validation data via hard voting...
Note: this may take some time! (~20 seconds)
  Time taken: 20.944220542907715s
  Accuracy: 0.9463785514205683
  F1-Score: 0.9463344019394132
   ```
---
## üßæ Authors
**<Ananya Kishore, DA24B035>**, IIT Madras (2025‚Äì26)
